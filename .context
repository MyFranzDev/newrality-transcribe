# Project Context Document

## Project Overview
**Name**: Newrality Transcribe
**Purpose**: Self-hosted audio transcription microservice using faster-whisper (Whisper Small model). Provides drop-in replacement for OpenAI Whisper API to reduce costs and maintain data privacy for VoiceFormFiller SaaS platform. Deployed as serverless GPU-accelerated service on Google Cloud Run.
**Demo URL**: https://newrality-transcribe-165455809003.us-central1.run.app
**Status**: Step 2 Completed - Deployed on Cloud Run

## Architecture (High-Level)

Microservizio Python standalone che espone un'API REST per trascrizione audio. Riceve file audio via multipart upload, li salva temporaneamente in memoria (tmpfs), esegue trascrizione con faster-whisper, e ritorna JSON strutturato compatibile con l'API esistente di VoiceFormFiller.

**Data Flow**:
1. Client (VoiceFormFiller Backend) invia audio via POST multipart
2. Microservizio valida formato e API key
3. Salva file in `/tmp` (effimero, solo RAM)
4. Esegue trascrizione con faster-whisper small model (GPU/CPU)
5. Ritorna JSON con testo trascritto
6. File eliminato automaticamente (stateless)

**Integration Context**:
- Questo microservizio sostituisce le chiamate a OpenAI Whisper API
- Il backend FastAPI di VoiceFormFiller chiamerà questo endpoint invece di OpenAI
- In futuro, il backend FastAPI gestirà storage long-term su Dreamhost SFTP (feature separata)

**Tech Stack**:
- **Framework**: FastAPI (Python 3.11+)
- **Trascrizione**: faster-whisper (CTranslate2)
- **Modello**: Whisper Small (244M params, 4x real-time, int8 quantization)
- **Deploy**: Google Cloud Run con GPU (T4)
- **Storage**: Tmpfs in-memory (stateless, no persistent disk)
- **Auth**: API Key (header X-API-Key)
- **Container**: Docker con CUDA support

## Credentials & Access

### Google Cloud Run (Production)
- **Project ID**: reportbabel
- **Service Name**: newrality-transcribe
- **Region**: us-central1
- **CLI**: `gcloud run deploy newrality-transcribe --region us-central1 --project=reportbabel`
- **Endpoint**: https://newrality-transcribe-165455809003.us-central1.run.app
- **Current Setup**: CPU-only (4GB RAM, 2 vCPU) with background model loading
- **GPU**: NVIDIA T4 support available (for future GPU deployment if needed)
- **Compute Type**: int8 quantization for speed/memory optimization

### GitHub
- **Repository**: https://github.com/MyFranzDev/newrality-transcribe
- **User**: francescozazza@gmail.com
- **Branch Strategy**: main for production, develop for development
- **CI/CD**: GitHub Actions for automated Cloud Run deploy (optional)

### External Dependencies
- **faster-whisper**: Python package (no API key needed)
- **Whisper Model**: Downloaded automatically on first run from Hugging Face
  - Model: `guillaumekln/faster-whisper-small`
  - Cached in container image for faster cold starts
- **API Key**: Shared secret for auth (environment variable)
  - Same key as VoiceFormFiller backend: `vff_dev_reportbabel_demo` (dev)
  - Production key: TBD (generate random secure key)

### Integration with VoiceFormFiller
- **Backend FastAPI**: https://github.com/MyFranzDev/ReportBabel
- **Endpoint to replace**: OpenAI Whisper API (`client.audio.transcriptions.create()`)
- **New endpoint**: `POST https://newrality-transcribe-[hash].run.app/api/v1/transcribe`
- **Request format**: Same as current (multipart/form-data with audio file)
- **Response format**: Compatible JSON structure

## Code Conventions

### General Principles
- **Microservice Design**: Single responsibility (only transcription)
- **Stateless**: No persistent storage, all data in-memory
- **Fail-fast**: Validate early, log errors clearly
- **Production-ready**: Proper error handling, structured logging, health checks
- **Cost-optimized**: Aggressive cold start optimization, minimal dependencies

### Python Conventions (PEP 8)
- **Naming**: `snake_case` for everything (functions, variables, files)
- **Modules**: Flat structure (no deep nesting)
- **Type hints**: Use for all function signatures
- **Logging**: Structured JSON logs with `structlog`
- **Errors**: Raise specific exceptions with context

### File Structure
```
newrality-transcribe/
├── .context                    # This file
├── README.md                   # Setup and usage docs
├── Dockerfile                  # Multi-stage build with CUDA
├── requirements.txt            # Python dependencies
├── .env.example                # Example environment variables
├── .gitignore                  # Standard Python gitignore
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI app entry point
│   ├── config.py               # Environment config
│   ├── models.py               # Pydantic request/response models
│   ├── transcription.py        # Whisper service logic
│   ├── auth.py                 # API key validation
│   └── utils.py                # Helpers (file validation, etc.)
├── scripts/
│   ├── deploy.sh               # Cloud Run deployment script
│   ├── test_local.sh           # Local testing with Docker
│   └── download_model.sh       # Pre-download model for Docker build
└── tests/
    ├── test_api.py             # API endpoint tests
    └── test_transcription.py   # Whisper service tests
```

### Best Practices
- **Environment variables** per tutta la configurazione (no hardcoded values)
- **Validation**: Validare formato audio, dimensione file (<25MB), codec supportati
- **Logging**: Log ogni richiesta con request_id per debugging
- **Timeouts**: Set timeout Cloud Run a 300s (max 5min per trascrizione)
- **Memory**: Request 2GB RAM + 16GB VRAM (GPU T4)
- **Cold start**: Pre-bake model in Docker image (~2GB), evita download on-demand
- **Error responses**: JSON strutturato con `error`, `detail`, `request_id`

## Cloud Services & Infrastructure

### Services to Create
- **Cloud Run Service**: `newrality-transcribe`
  - Region: us-central1
  - GPU: 1x NVIDIA T4
  - Memory: 2GB RAM
  - CPU: 2 vCPU
  - Timeout: 300s (5 minutes)
  - Concurrency: 1 (one request per container, GPU exclusive)
  - Min instances: 0 (scale to zero for cost saving)
  - Max instances: 5 (limit concurrent GPU usage)
  - Ingress: All (public endpoint with API key auth)

- **Secret Manager** (optional, for API key):
  - Secret: `transcribe-api-key`
  - Value: Generated secure random key

- **Container Registry**:
  - Registry: Artifact Registry (recommended) or Container Registry
  - Image: `gcr.io/[project-id]/newrality-transcribe:latest`

### Service Endpoints
- **Base URL**: `https://newrality-transcribe-[hash].run.app/api/v1`
- **Endpoints**:
  - `GET /health` - Health check (no auth)
    - Returns: `{"status": "healthy", "model": "small", "device": "cuda"}`
  - `POST /transcribe` - Audio transcription (requires API key)
    - Headers: `X-API-Key: [key]`
    - Body: `multipart/form-data` with `file` field
    - Query params (optional):
      - `language` (default: "it")
      - `temperature` (default: 0.0)
      - `beam_size` (default: 5)
    - Returns: `{"text": "...", "language": "it", "duration": 1.23}`
  - `GET /models` - List available models (no auth)
    - Returns: `{"models": ["small"], "active": "small"}`

### Cost Estimation (MVP Phase)
- **Cloud Run GPU (T4)**: ~$0.50/hour runtime
- **Storage**: ~2GB image in Artifact Registry (~$0.10/month)
- **Network**: Egress costs minimal for JSON responses
- **Estimated**: ~$5-20/month for low-volume testing (scale to zero helps)
- **Production**: ~$50-200/month for moderate usage (100 requests/day)

## Roadmap

### Step 1: Local Development & Testing
**Status**: ✅ Completed (October 19, 2025)
**Goal**: Build working microservice locally with faster-whisper, validate on sample audio files
- [x] Initialize Python project with FastAPI + faster-whisper
- [x] Implement `/transcribe` endpoint with file upload and validation
- [x] Test faster-whisper small model locally (CPU mode first)
- [x] Add structured logging with request IDs
- [x] Implement API key authentication
- [x] Create health check endpoint
- [x] Write unit tests for core functionality
- [x] Test with VoiceFormFiller sample audio files (tested with real medical audio in English and Italian)

### Step 2: Dockerization & Cloud Run Deployment
**Status**: ✅ Completed (October 19, 2025)
**Goal**: Create production-ready Docker image with CUDA support and deploy to Cloud Run
- [x] Write Dockerfile with CUDA base image (single-stage)
- [x] Pre-download Whisper Small model during build (avoid runtime download)
- [x] Implement background model loading to avoid startup timeout
- [x] Create deployment scripts (download_model.py, .dockerignore)
- [x] Deploy to Google Cloud Run (project: reportbabel)
- [x] Test deployed service with real medical audio
- [x] Optimize cold start time (<5s with background loading)

### Step 3: VoiceFormFiller Backend Integration
**Status**: Not Started
**Goal**: Integrate newrality-transcribe endpoint into VoiceFormFiller backend with fallback
- [ ] Add environment variables to VoiceFormFiller backend (TRANSCRIBE_SERVICE_URL, TRANSCRIBE_API_KEY)
- [ ] Implement transcription client wrapper in VoiceFormFiller
- [ ] Add feature flag: USE_SELF_HOSTED_WHISPER (default: false)
- [ ] Implement fallback logic (try self-hosted → fallback to OpenAI on error)
- [ ] Test end-to-end integration with Report Babel frontend
- [ ] Monitor transcription accuracy comparison (self-hosted vs OpenAI)
- [ ] Gradual rollout: 10% → 50% → 100% traffic to self-hosted
- [ ] Document cost savings achieved

### Step 4: Production Hardening & Monitoring
**Status**: Not Started
**Goal**: Add production-grade features: monitoring, rate limiting, error handling
- [ ] Add rate limiting (per API key, e.g., 100 requests/hour)
- [ ] Implement request timeout handling (graceful failures)
- [ ] Add Cloud Logging integration with structured logs
- [ ] Set up Cloud Monitoring dashboard (request count, latency, errors)
- [ ] Add error alerting (email/Slack on high error rate)
- [ ] Create deployment script with automated testing
- [ ] Document rollback procedure
- [ ] Load test with concurrent requests (validate GPU concurrency=1 behavior)
- [ ] Create runbook for common issues

### Step 5: Cost Optimization & Scaling
**Status**: Not Started
**Goal**: Optimize costs and prepare for SaaS scale
- [ ] Analyze Cloud Run logs for actual usage patterns
- [ ] Tune cold start optimization (model caching strategies)
- [ ] Evaluate CPU-only mode for lower-priority requests (cost saving)
- [ ] Implement request queuing for burst traffic
- [ ] Add autoscaling policies based on usage
- [ ] Create cost monitoring dashboard
- [ ] Document cost per transcription metric
- [ ] Evaluate regional deployment for latency (EU regions)

### Step 6: VoiceFormFiller Backend Integration
**Status**: Not Started
**Goal**: Fully migrate VoiceFormFiller from OpenAI to newrality-transcribe with fallback
- [ ] Add feature flag in VoiceFormFiller: `USE_SELF_HOSTED_WHISPER`
- [ ] Implement fallback logic (try self-hosted, fallback to OpenAI on error)
- [ ] Update environment variables in VoiceFormFiller backend
- [ ] Test fallback behavior (self-hosted down → OpenAI used)
- [ ] Monitor accuracy comparison (self-hosted vs OpenAI)
- [ ] Gradual rollout: 10% → 50% → 100% traffic to self-hosted
- [ ] Remove OpenAI dependency after stable production usage
- [ ] Document cost savings achieved

### Step 7: Advanced Features (Future SaaS)
**Status**: Not Started
**Goal**: Add advanced features for multi-tenant SaaS platform
- [ ] Multi-model support (switch between tiny/base/small/medium dynamically)
- [ ] Language detection before transcription (optimize model selection)
- [ ] Speaker diarization support (who-spoke-when)
- [ ] Timestamp output (word-level or segment-level)
- [ ] Webhook callbacks for async transcription
- [ ] Batch transcription endpoint (multiple files)
- [ ] Custom vocabulary injection (medical terms, brand names)
- [ ] Model fine-tuning pipeline for domain-specific accuracy

## Commit History

### October 19, 2025 - Step 1 Implementation

**Commit 1: 86ec195** - Initial commit: project context
- Added `.context` file with project overview, architecture, and roadmap
- Defined Step 1-7 implementation plan
- Documented tech stack, credentials, and integration points

**Commit 2: 5cb6450** - Implement FastAPI microservice with faster-whisper transcription
- **Files Added:** 11 files, 1,097 lines of code
- **Core Implementation:**
  - `app/main.py` (234 lines) - FastAPI application with 3 endpoints
  - `app/transcription.py` (154 lines) - faster-whisper integration
  - `app/config.py` (93 lines) - Pydantic Settings configuration
  - `app/models.py` (73 lines) - Request/response schemas
  - `app/auth.py` (36 lines) - API key authentication
  - `app/utils.py` (119 lines) - File validation utilities
- **Documentation:**
  - `README.md` (272 lines) - Complete setup and usage guide
  - `.env.example` (25 lines) - Configuration template
- **Infrastructure:**
  - `requirements.txt` (22 lines) - Python dependencies
  - `.gitignore` (66 lines) - Git ignore rules
- **Testing:** Whisper Small model loaded successfully (77s), all endpoints functional

**Commit 3: 658913e** - Fix NumPy compatibility issue for onnxruntime
- **Issue:** NumPy 2.0 caused segmentation fault with onnxruntime VAD filter
- **Solution:** Pinned numpy to version <2 in requirements.txt
- **Impact:** Enabled Voice Activity Detection for improved transcription accuracy
- **Testing:** Verified with English and Italian medical audio files

**Commit 4: 03520b1** - chore: update report timestamp [skip ci]
- Created `.last-report-timestamp` for tracking project status reports
- Automated by Julia reporter agent after email delivery

**Commit 5: 73f66b1** - Update .context: Mark Step 1 as completed
- Updated project status to reflect Step 1 completion
- Added comprehensive commit history section
- Documented all implementation details from Step 1

**Commit 6: 50b2cbb** - Implement background model loading and Docker support
- **Files Added/Modified:** 5 files, 225 insertions, 21 deletions
- **Background Model Loading:**
  - Modified `app/transcription.py` with async model loading using threading
  - Added `load_model_async()` and `wait_for_model()` methods
  - FastAPI server starts in ~2-3s, model loads in parallel (~30-60s)
- **Health Check Improvements:**
  - Granular status: starting → loading → ready → unhealthy
  - Immediate response even during model loading
- **Docker Support:**
  - Single-stage Dockerfile with CUDA base (Python 3.11, Ubuntu 22.04)
  - Pre-downloads Whisper Small model (~926MB) during build
  - Created `scripts/download_model.py` for model pre-download
  - Created `.dockerignore` to optimize build context
- **Deployment:**
  - Successfully deployed to Cloud Run (project: reportbabel, region: us-central1)
  - Service URL: https://newrality-transcribe-165455809003.us-central1.run.app
  - Configuration: 4GB RAM, 2 vCPU, CPU-only (GPU T4 available for future)
  - Tested with real medical audio transcription - working correctly
  - No cold start timeout issues

### Summary
- **Total Commits:** 6
- **Total Lines Added:** 1,300+
- **Test Status:** ✅ All endpoints tested locally and on Cloud Run
- **Audio Transcription:** ✅ Validated with real medical audio (English & Italian)
- **Deployment Status:** ✅ Live on Cloud Run (CPU-only, background loading)

---

## Notes for Claude Code

### Before Starting Work
- Read this entire `.context` document first
- Verify you have access to the VoiceFormFiller backend codebase for integration reference
- Check if Google Cloud project exists or needs creation
- Confirm faster-whisper model can be downloaded (network access)

### During Development
- Test every endpoint manually with curl before moving to next step
- Log everything with structured JSON for debugging
- Commit after each working feature (granular commits)
- Update Roadmap status as tasks are completed
- Document any deviations from plan in commit messages

### Testing Strategy
1. **Unit tests**: Test transcription logic with mock audio
2. **Integration tests**: Test full API with real audio samples
3. **Local Docker tests**: Validate Dockerfile builds and runs
4. **Cloud Run tests**: Test deployed service end-to-end
5. **VoiceFormFiller integration**: Test from actual client

### Key Design Decisions Already Made
- ✅ faster-whisper (not OpenAI Whisper or whisper.cpp)
- ✅ Whisper Small model (not tiny/base/medium/large)
- ✅ Google Cloud Run with GPU T4 (not CPU-only or VM)
- ✅ Separate microservice (not integrated in VoiceFormFiller backend)
- ✅ API Key authentication (not OAuth or IAM-only)
- ✅ Stateless design (no persistent storage in microservice)
- ✅ In-memory tmpfs for audio files (not Cloud Storage)
- ✅ HTTP REST API (not gRPC or Pub/Sub)

### Common Pitfalls to Avoid
- ❌ Don't use OpenAI Whisper (use faster-whisper)
- ❌ Don't add persistent storage (keep stateless)
- ❌ Don't download model on every request (bake in Docker image)
- ❌ Don't deploy without GPU (T4 required for production speed)
- ❌ Don't forget API key validation (security critical)
- ❌ Don't skip health check endpoint (monitoring needs it)
- ❌ Don't use concurrency >1 with GPU (GPU is exclusive)

### Useful Resources
- faster-whisper docs: https://github.com/SYSTRAN/faster-whisper
- Cloud Run GPU guide: https://cloud.google.com/run/docs/configuring/services/gpu
- VoiceFormFiller backend: `/Users/francesco/newrality/ReportBabel/backend`
- OpenAI Whisper API reference: https://platform.openai.com/docs/api-reference/audio

### When Stuck
- Check VoiceFormFiller backend code for current OpenAI integration
- Review Cloud Run logs for deployment errors
- Test faster-whisper model locally first (isolate issues)
- Validate Docker image runs locally before Cloud Run deploy
- Consult faster-whisper GitHub issues for common problems

### Demo URL
✅ Cloud Run service is now deployed and live:
`https://newrality-transcribe-165455809003.us-central1.run.app`

Save this URL in VoiceFormFiller backend `.env` as:
```bash
TRANSCRIBE_SERVICE_URL=https://newrality-transcribe-165455809003.us-central1.run.app/api/v1
TRANSCRIBE_API_KEY=vff_dev_reportbabel_demo
```

**Test the service:**
```bash
# Health check
curl https://newrality-transcribe-165455809003.us-central1.run.app/health

# Transcription (replace with your audio file)
curl -X POST "https://newrality-transcribe-165455809003.us-central1.run.app/api/v1/transcribe?language=en" \
  -H "X-API-Key: vff_dev_reportbabel_demo" \
  -F "file=@/path/to/audio.m4a"
```

### Email Reporting (Julia Bakker)

**⚠️ PROCEDURA CORRETTA:**
1. ✅ **INVOCARE agente Julia** (06-julia-reporter) per mandare email
2. ✅ Julia ha il contesto giusto per scrivere il contenuto dell'email
3. ✅ Julia usa automaticamente lo script `/tmp/send_newrality_report.py` (già configurato correttamente)
4. ❌ NON inviare email direttamente senza Julia - lei ha il contesto del progetto

**Script utilizzato da Julia:**
- `/tmp/send_newrality_report.py` - Già aggiornato con configurazione corretta
- Julia modificherà il contenuto dell'email in base al contesto del progetto
- Lo script ha già header corretti, multipart, e firma HTML

**Configurazione email corretta (già implementata nello script):**
- ✅ Subject SOBRIO senza emoji (es: "Progetto Transcribe - Step X Completato")
- ✅ Header completi: From con nome "Julia Bakker <julia.bakker@newrality.com>"
- ✅ Reply-To: julia.bakker@newrality.com
- ✅ Multipart: HTML + Plain Text
- ✅ Emoji: SI nel corpo dell'email, NO nel subject
- ✅ Firma: HTML con immagini (Julia + logo Newrality)

**Credenziali SMTP:**
- Server: smtp.dreamhost.com:587
- Account: julia.bakker@newrality.com
- Password: lb4g$kh$%NYyyAP@
- Destinatario: francesco.zazza@newrality.com
